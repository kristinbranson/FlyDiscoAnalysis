{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import math\n",
    "import cv2\n",
    "import tempfile\n",
    "import copy\n",
    "import re\n",
    "import h5py\n",
    "\n",
    "from batch_norm import *\n",
    "import myutils\n",
    "import PoseTools\n",
    "import localSetup\n",
    "import operator\n",
    "import copy\n",
    "import convNetBase as CNB\n",
    "import multiResData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(X, kernel_shape, conv_std,bias_val,doBatchNorm,trainPhase,addSummary=True):\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "                              #tf.random_normal_initializer(stddev=conv_std))\n",
    "    biases = tf.get_variable(\"biases\", kernel_shape[-1],\n",
    "        initializer=tf.constant_initializer(bias_val))\n",
    "    if addSummary:\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "#     PoseTools.variable_summaries(biases)\n",
    "    conv = tf.nn.conv2d(X, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if doBatchNorm:\n",
    "        conv = batch_norm(conv,trainPhase)\n",
    "    with tf.variable_scope('conv'):\n",
    "        PoseTools.variable_summaries(conv)\n",
    "    return tf.nn.relu(conv - biases)\n",
    "\n",
    "def conv_relu_norm_init(X, kernel_shape, conv_std,bias_val,doBatchNorm,trainPhase,addSummary=True):\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer(stddev=conv_std))\n",
    "    biases = tf.get_variable(\"biases\", kernel_shape[-1],\n",
    "        initializer=tf.constant_initializer(bias_val))\n",
    "    if addSummary:\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "#     PoseTools.variable_summaries(biases)\n",
    "    conv = tf.nn.conv2d(X, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if doBatchNorm:\n",
    "        conv = batch_norm(conv,trainPhase)\n",
    "    with tf.variable_scope('conv'):\n",
    "        PoseTools.variable_summaries(conv)\n",
    "    return tf.nn.relu(conv - biases)\n",
    "\n",
    "def FC_2D(S,nfilt,trainPhase,addSummary=True):\n",
    "    \n",
    "    inDim = S.get_shape()[1]\n",
    "    weights = tf.get_variable(\"weights\", [inDim,nfilt],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", nfilt,\n",
    "        initializer=tf.constant_initializer(0))\n",
    "    if addSummary:\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "    fc_out = tf.nn.relu(batch_norm_2D(tf.matmul(S, weights),trainPhase)-biases)\n",
    "    with tf.variable_scope('fc'):\n",
    "        PoseTools.variable_summaries(fc_out)\n",
    "    return fc_out\n",
    "\n",
    "def FC_2D_norm_init(S,nfilt,trainPhase,conv_std,addSummary=True):\n",
    "    \n",
    "    inDim = S.get_shape()[1]\n",
    "    weights = tf.get_variable(\"weights\", [inDim,nfilt],\n",
    "        initializer=tf.random_normal_initializer(stddev=conv_std))\n",
    "    biases = tf.get_variable(\"biases\", nfilt,\n",
    "        initializer=tf.constant_initializer(0))\n",
    "    if addSummary:\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "    fc_out = tf.nn.relu(batch_norm_2D(tf.matmul(S, weights),trainPhase)-biases)\n",
    "    with tf.variable_scope('fc'):\n",
    "        PoseTools.variable_summaries(fc_out)\n",
    "    return fc_out\n",
    "\n",
    "\n",
    "def max_pool(name, l_input, k,s):\n",
    "    return tf.nn.max_pool(\n",
    "        l_input, ksize=[1, k, k, 1], strides=[1, s, s, 1], \n",
    "        padding='SAME', name=name)\n",
    "\n",
    "def createPlaceHolders(conf):\n",
    "#     imsz = conf.imsz\n",
    "    # tf Graph input\n",
    "    imsz = conf.imsz\n",
    "    psz = conf.poseEval2_psz\n",
    "    rescale = conf.rescale\n",
    "    scale = conf.scale\n",
    "    pool_scale = conf.pool_scale\n",
    "    n_classes = conf.n_classes\n",
    "    img_dim = conf.img_dim\n",
    "    \n",
    "    inScale = rescale\n",
    "    \n",
    "    nex = conf.batch_size*(conf.eval_num_neg+1)\n",
    "    x0 = []\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    s0 = []\n",
    "    a0 = []\n",
    "    d0 = []\n",
    "#change 2 21022017    \n",
    "    \n",
    "    for ndx in range(1): #n_classes):\n",
    "        x0.append(tf.placeholder(tf.float32, [nex,psz,psz,img_dim],name='x0_{}'.format(ndx)))\n",
    "        x1.append(tf.placeholder(tf.float32, [nex,psz,psz,img_dim],name='x1_{}'.format(ndx)))\n",
    "        x2.append(tf.placeholder(tf.float32, [nex,psz,psz,img_dim],name='x2_{}'.format(ndx)))\n",
    "        s0.append(tf.placeholder(tf.float32, [nex,2*n_classes],name='s0_{}'.format(ndx)))\n",
    "        a0.append(tf.placeholder(tf.float32, [nex,8*n_classes],name='a0_{}'.format(ndx)))\n",
    "        d0.append(tf.placeholder(tf.float32, [nex,n_classes],name='d0_{}'.format(ndx)))\n",
    "#change 2 21022017    \n",
    "    \n",
    "#change 3 22022017    \n",
    "    y = tf.placeholder(tf.float32, [nex,8*n_classes],'out')\n",
    "#     y = tf.placeholder(tf.float32, [nex,n_classes,],'out')\n",
    "    \n",
    "    X = [x0,x1,x2]\n",
    "    S = [s0,a0,d0]\n",
    "#change 2 21022017    \n",
    "    \n",
    "    phase_train = tf.placeholder(tf.bool,name='phase_train')\n",
    "    learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    \n",
    "    ph = {'X':X,'S':S,'y':y,\n",
    "          'phase_train':phase_train,'learning_rate':learning_rate}\n",
    "    return ph\n",
    "\n",
    "\n",
    "def createFeedDict(ph,conf):\n",
    "    feed_dict = {}\n",
    "    for ndx in range(1): #conf.n_classes):\n",
    "        feed_dict[ph['X'][0][ndx]] = []\n",
    "        feed_dict[ph['X'][1][ndx]] = []\n",
    "        feed_dict[ph['X'][2][ndx]] = []\n",
    "        feed_dict[ph['S'][0][ndx]] = []\n",
    "        feed_dict[ph['S'][1][ndx]] = []\n",
    "        feed_dict[ph['S'][2][ndx]] = []\n",
    "#change 2 21022017        \n",
    "    feed_dict[ph['y']]=[]\n",
    "    feed_dict[ph['learning_rate']]=1\n",
    "    feed_dict[ph['phase_train']]=False\n",
    "    \n",
    "    return feed_dict\n",
    "\n",
    "def net_multi_base_named(X,nfilt,doBatchNorm,trainPhase,addSummary=True):\n",
    "    inDimX = X.get_shape()[3]\n",
    "    nex = X.get_shape()[0].value\n",
    "    \n",
    "    with tf.variable_scope('layer1_X'):\n",
    "        conv1 = conv_relu_norm_init(X,[5, 5, inDimX, 48],0.3,0,doBatchNorm,trainPhase,addSummary)\n",
    "#         pool1 = max_pool('pool1',conv1,k=3,s=2)\n",
    "        pool1 = conv1\n",
    "            \n",
    "    with tf.variable_scope('layer2'):\n",
    "        conv2 = conv_relu(pool1,[3,3,48,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "#         pool2 = max_pool('pool2',conv2,k=3,s=2)\n",
    "        pool2 = conv2\n",
    "            \n",
    "    with tf.variable_scope('layer3'):\n",
    "        conv3 = conv_relu(pool2,[3,3,nfilt,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "        pool3 = max_pool('pool3',conv3,k=3,s=2)\n",
    "        \n",
    "    with tf.variable_scope('layer4'):\n",
    "        conv4 = conv_relu(pool3,[3,3,nfilt,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "        pool4 = max_pool('pool4',conv4,k=3,s=2)\n",
    "    conv4_reshape = tf.reshape(pool4,[nex,-1])\n",
    "    conv4_dims = conv4_reshape.get_shape()[1].value\n",
    "        \n",
    "    with tf.variable_scope('layer5'):\n",
    "        conv5 = FC_2D_norm_init(conv4_reshape,128,trainPhase,0.01,addSummary)\n",
    "        \n",
    "    out_dict = {'conv1':conv1,'conv2':conv2,'conv3':conv3,\n",
    "               'conv4':conv4,'conv5':conv5}\n",
    "    return conv5,out_dict\n",
    "        \n",
    "def net_pose(S,nfilt,trainPhase):\n",
    "    inDim = S.get_shape()[1]\n",
    "    with tf.variable_scope('layer1_pose'):\n",
    "        L1 = FC_2D(S,nfilt,trainPhase,True)\n",
    "    with tf.variable_scope('layer2_pose'):\n",
    "        L2 = FC_2D(L1,nfilt,trainPhase,True)\n",
    "    with tf.variable_scope('layer3_pose'):\n",
    "        L3 = FC_2D(L2,nfilt,trainPhase,True)\n",
    "    out_dict = {'L1':L1,'L2':L2,'L3':L3}\n",
    "    return L3,out_dict\n",
    "    \n",
    "def net_multi_conv(ph,conf):\n",
    "    X = ph['X']\n",
    "    S = ph['S']\n",
    "    X0,X1,X2 = X\n",
    "    S0,A0,D0 = S\n",
    "    \n",
    "    out_size = ph['y'].get_shape()[1]\n",
    "    trainPhase = ph['phase_train']\n",
    "    \n",
    "    imsz = conf.imsz; rescale = conf.rescale\n",
    "    pool_scale = conf.pool_scale\n",
    "    nfilt = conf.nfilt\n",
    "    doBatchNorm = conf.doBatchNorm\n",
    "    \n",
    "    L7_array = []\n",
    "    base_dict_array = []\n",
    "    for ndx in range(conf.n_classes):\n",
    "    \n",
    "        with tf.variable_scope('scale0_{}'.format(ndx)):\n",
    "            conv5_0,base_dict_0 = net_multi_base_named(X0[ndx],nfilt,doBatchNorm,trainPhase,True)\n",
    "        with tf.variable_scope('scale1_{}'.format(ndx)):\n",
    "            conv5_1,base_dict_1 = net_multi_base_named(X1[ndx],nfilt,doBatchNorm,trainPhase,False)\n",
    "        with tf.variable_scope('scale2_{}'.format(ndx)):\n",
    "            conv5_2,base_dict_2 = net_multi_base_named(X2[ndx],nfilt,doBatchNorm,trainPhase,False)\n",
    "        with tf.variable_scope('pose_s_fc_{}'.format(ndx)):\n",
    "            L3_S,s_dict = net_pose(S0[ndx],nfilt,trainPhase)\n",
    "        with tf.variable_scope('pose_a_fc_{}'.format(ndx)):\n",
    "            L3_A,a_dict = net_pose(A0[ndx],nfilt,trainPhase)\n",
    "        with tf.variable_scope('pose_d_fc_{}'.format(ndx)):\n",
    "            L3_D,d_dict = net_pose(D0[ndx],nfilt,trainPhase)\n",
    "        \n",
    "#         conv5_cat = tf.concat(1,[conv5_0,conv5_1,conv5_2,L3_S,L3_A,L3_D])\n",
    "#change 1 20022017\n",
    "        conv5_cat = tf.concat(1,[L3_S/3,L3_A/3,L3_D/3])\n",
    "    \n",
    "        with tf.variable_scope('L6_{}'.format(ndx)):\n",
    "            L6 = FC_2D(conv5_cat,128,trainPhase)\n",
    "        with tf.variable_scope('L7_{}'.format(ndx)):\n",
    "            L7 = FC_2D(L6,32,trainPhase)\n",
    "        L7_array.append(L7)\n",
    "        base_dict_array.append([base_dict_0,base_dict_2,base_dict_2,L6,L7])\n",
    "    \n",
    "    L7_cat = tf.concat(1,L7_array)\n",
    "    with tf.variable_scope('L8'.format(ndx)):\n",
    "        L8 = FC_2D(L7_cat,128,trainPhase)\n",
    "        \n",
    "    with tf.variable_scope('out'.format(ndx)):\n",
    "        weights = tf.get_variable(\"weights\", [L8.get_shape()[1].value,out_size],\n",
    "            initializer=tf.random_normal_initializer(stddev=.05)) # tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable(\"biases\", out_size,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "        out = tf.matmul(L8, weights)-biases\n",
    "        with tf.variable_scope('conv'):\n",
    "            PoseTools.variable_summaries(out)\n",
    "\n",
    "    out_dict = {'base_dict_array':base_dict_array,\n",
    "                'L8':L8}\n",
    "    \n",
    "    return out,out_dict\n",
    "\n",
    "def net_multi_conv_shape(ph,conf):\n",
    "    X = ph['X']\n",
    "    X0,X1,X2 = X\n",
    "    \n",
    "    out_size = ph['y'].get_shape()[1]\n",
    "    trainPhase = ph['phase_train']\n",
    "    \n",
    "    imsz = conf.imsz; rescale = conf.rescale\n",
    "    pool_scale = conf.pool_scale\n",
    "    nfilt = conf.nfilt\n",
    "    doBatchNorm = conf.doBatchNorm\n",
    "    \n",
    "    L7_array = []\n",
    "    base_dict_array = []\n",
    "    for ndx in range(1):\n",
    "    \n",
    "        with tf.variable_scope('scale0_{}'.format(ndx)):\n",
    "            conv5_0,base_dict_0 = net_multi_base_named(X0[ndx],nfilt,doBatchNorm,trainPhase,True)\n",
    "        with tf.variable_scope('scale1_{}'.format(ndx)):\n",
    "            conv5_1,base_dict_1 = net_multi_base_named(X1[ndx],nfilt,doBatchNorm,trainPhase,False)\n",
    "        with tf.variable_scope('scale2_{}'.format(ndx)):\n",
    "            conv5_2,base_dict_2 = net_multi_base_named(X2[ndx],nfilt,doBatchNorm,trainPhase,False)\n",
    "        conv5_cat = tf.concat(1,[conv5_0,conv5_1,conv5_2])\n",
    "    \n",
    "        with tf.variable_scope('L6_{}'.format(ndx)):\n",
    "            L6 = FC_2D(conv5_cat,128,trainPhase)\n",
    "        with tf.variable_scope('L7_{}'.format(ndx)):\n",
    "            L7 = FC_2D(L6,128,trainPhase)\n",
    "        L7_array.append(L7)\n",
    "        base_dict_array.append([base_dict_0,base_dict_2,base_dict_2,L6,L7])\n",
    "    \n",
    "    L7_cat = tf.concat(1,L7_array)\n",
    "    with tf.variable_scope('L8'.format(ndx)):\n",
    "        L8 = FC_2D(L7_cat,128,trainPhase)\n",
    "        \n",
    "    with tf.variable_scope('out'.format(ndx)):\n",
    "        weights = tf.get_variable(\"weights\", [L8.get_shape()[1].value,out_size],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.2))\n",
    "        biases = tf.get_variable(\"biases\", out_size,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "        out = tf.matmul(L8, weights)-biases\n",
    "        with tf.variable_scope('conv'):\n",
    "            PoseTools.variable_summaries(out)\n",
    "\n",
    "    out_dict = {'base_dict_array':base_dict_array,\n",
    "                'L8':L8}\n",
    "    \n",
    "    return out,out_dict\n",
    "\n",
    "\n",
    "def openDBs(conf,trainType=0):\n",
    "        if trainType == 0:\n",
    "            trainfilename =os.path.join(conf.cachedir,conf.trainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(conf.cachedir,conf.valfilename) + '.tfrecords'\n",
    "            train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            val_queue = tf.train.string_input_producer([valfilename])\n",
    "        else:\n",
    "            trainfilename =os.path.join(conf.cachedir,conf.fulltrainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(conf.cachedir,conf.fulltrainfilename) + '.tfrecords'\n",
    "            train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            val_queue = tf.train.string_input_producer([valfilename])\n",
    "        return [train_queue,val_queue]\n",
    "\n",
    "def createCursors(sess,queue,conf):\n",
    "            \n",
    "        train_queue,val_queue = queue\n",
    "        train_ims,train_locs,temp = multiResData.read_and_decode(train_queue,conf)\n",
    "        val_ims,val_locs,temp = multiResData.read_and_decode(val_queue,conf)\n",
    "        train_data = [train_ims,train_locs]\n",
    "        val_data = [val_ims,val_locs]\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "        return [train_data,val_data],coord,threads\n",
    "        \n",
    "\n",
    "def readImages(conf,dbType,distort,sess,data):\n",
    "    train_data,val_data = data\n",
    "    cur_data = val_data if (dbType=='val') \\\n",
    "            else train_data\n",
    "    xs = []; locs = []\n",
    "\n",
    "    count = 0\n",
    "    while count < conf.batch_size:\n",
    "        [curxs,curlocs] = sess.run(cur_data)\n",
    "        \n",
    "        kk = curlocs[conf.eval2_selpt2,:]-curlocs[conf.eval2_selpt1,:]\n",
    "        dd = np.sqrt(kk[0]**2 + kk[1]**2 )\n",
    "#         if dd>150:\n",
    "#             continue\n",
    "\n",
    "        if np.ndim(curxs)<3:\n",
    "            xs.append(curxs[np.newaxis,:,:])\n",
    "        else:\n",
    "            xs.append(curxs)\n",
    "        locs.append(curlocs)\n",
    "        count = count+1\n",
    "    \n",
    "        \n",
    "    xs = np.array(xs)    \n",
    "    locs = np.array(locs)\n",
    "    locs = multiResData.sanitizelocs(locs)\n",
    "    if distort:\n",
    "        if conf.horz_flip:\n",
    "            xs,locs = PoseTools.randomlyFlipLR(xs,locs)\n",
    "        if conf.vert_flip:\n",
    "            xs,locs = PoseTools.randomlyFlipUD(xs,locs)\n",
    "        xs,locs = PoseTools.randomlyRotate(xs,locs,conf)\n",
    "        xs = PoseTools.randomlyAdjust(xs,conf)\n",
    "\n",
    "    return xs,locs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateFeedDict(conf,dbType,distort,sess,data,feed_dict,ph):\n",
    "    xs,locs = readImages(conf,dbType,distort,sess,data)\n",
    "#     global shape_prior\n",
    "    \n",
    "    minlen = conf.eval_minlen\n",
    "    nlocs = genNegSamples(locs,conf,minlen=minlen)\n",
    "    alllocs = nlocs\n",
    "    dd = alllocs-locs # distance of neg points to actual locations\n",
    "    ddist = np.sqrt(np.sum(dd**2,axis=2))\n",
    "\n",
    "    selpt1 = conf.eval2_selpt1\n",
    "    selpt2 = conf.eval2_selpt2\n",
    "    ind_labels = shape_from_locs(locs)\n",
    "    ind_labels = ind_labels[:,selpt1,:,...]\n",
    "#     ind_labels = ind_labels/(shape_prior[selpt1,selpt2,...]+0.1)\n",
    "#change 3 22022017,orig below\n",
    "    \n",
    "#     ind_labels = ddist/minlen/2\n",
    "#     ind_labels = 1-2*ind_labels\n",
    "#     ind_labels = ind_labels.clip(min=-1,max=1)\n",
    "\n",
    "    psz = conf.poseEval2_psz\n",
    "    x0,x1,x2 = PoseTools.multiScaleImages(xs.transpose([0,2,3,1]),\n",
    "                                          conf.rescale,conf.scale, conf.l1_cropsz,conf)\n",
    "#     jj = np.random.randint(3)\n",
    "#change 3 22022017\n",
    "    jj = 2\n",
    "    if jj is 0:\n",
    "        locs_patch = nlocs\n",
    "        locs_coords = nlocs\n",
    "    elif jj is 1:\n",
    "        locs_patch = nlocs\n",
    "        locs_coords = locs\n",
    "    elif jj is 2:\n",
    "        locs_patch = locs\n",
    "        locs_coords = nlocs\n",
    "        \n",
    "    ang = angle_from_locs(locs_coords)\n",
    "    dist = dist_from_locs(locs_coords)\n",
    "    for ndx,count in enumerate(range(selpt1,selpt1+1)): #conf.n_classes):\n",
    "        feed_dict[ph['X'][0][ndx]] = extract_patches(x0,locs_patch[:,count,:],psz)\n",
    "        feed_dict[ph['X'][1][ndx]] = extract_patches(x1,(locs_patch[:,count,:])/conf.scale,psz)\n",
    "        feed_dict[ph['X'][2][ndx]] = extract_patches(x2,(locs_patch[:,count,:])/(conf.scale**2),psz)\n",
    "        feed_dict[ph['S'][0][ndx]] = np.reshape(locs_coords-locs_coords[:,ndx:ndx+1,:],[conf.batch_size,2*conf.n_classes])\n",
    "        feed_dict[ph['S'][1][ndx]] = np.reshape(ang[:,ndx,:,:],[conf.batch_size,8*conf.n_classes])\n",
    "        feed_dict[ph['S'][2][ndx]] = dist[:,ndx,:]\n",
    "    feed_dict[ph['y']] = np.reshape(ind_labels,[conf.batch_size,-1])\n",
    "    return alllocs,locs,xs\n",
    "\n",
    "def angle_from_locs(locs):\n",
    "    norts = 8\n",
    "    npts = locs.shape[1]\n",
    "    bsz = locs.shape[0]\n",
    "    yy = np.zeros([bsz,npts,npts,norts])\n",
    "    for ndx in range(bsz):\n",
    "        curl = locs[ndx,...]\n",
    "        rloc = np.tile(curl, [npts,1,1] )\n",
    "        kk = rloc - curl[:,np.newaxis,:]\n",
    "        aa = np.arctan2( kk[:,:,1], kk[:,:,0]+1e-5 )*180/np.pi + 180\n",
    "        for i in range(norts):\n",
    "            pndx = np.abs(np.mod(aa-360/norts*i+45,360)-45-22.5)<32.5\n",
    "            zz = np.zeros([npts,npts])\n",
    "            zz[pndx] = 1\n",
    "            yy[ndx,...,i] = zz\n",
    "    return yy\n",
    "\n",
    "def dist_from_locs(locs):\n",
    "    npts = locs.shape[1]\n",
    "    bsz = locs.shape[0]\n",
    "    yy = np.zeros([bsz,npts,npts])\n",
    "    for ndx in range(bsz):\n",
    "        curl = locs[ndx,...]\n",
    "        dd = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(curl))\n",
    "        yy[ndx,...] = dd\n",
    "    return yy\n",
    "\n",
    "def shape_from_locs(locs):\n",
    "    # shape context kinda labels \n",
    "    n_angle = 8\n",
    "#     r_bins = np.logspace(4,10,n_radius-1,base=2)\n",
    "#     r_bins = np.concatenate([[0,],r_bins,[np.inf,]])\n",
    "#     r_bins = np.array([0,64,128,256,np.inf])\n",
    "    r_bins = np.array([0,np.inf])\n",
    "    n_radius = len(r_bins)-1\n",
    "#     r_bins_high = np.array([90,150,290,inf])\n",
    "    npts = locs.shape[1]\n",
    "    bsz = locs.shape[0]\n",
    "    yy = np.zeros([bsz,npts,npts,n_angle,n_radius])\n",
    "    for ndx in range(bsz):\n",
    "        curl = locs[ndx,...]\n",
    "        dd = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(curl))\n",
    "        dd_bins = np.digitize(dd,r_bins)\n",
    "        rloc = np.tile(curl, [npts,1,1] )\n",
    "        kk = rloc - curl[:,np.newaxis,:]\n",
    "        aa = np.arctan2( kk[:,:,1], kk[:,:,0]+1e-5 )*180/np.pi + 180\n",
    "        \n",
    "        for i in range(n_angle):\n",
    "            for dbin in range(n_radius):\n",
    "                pndx = np.abs(np.mod(aa-360/n_angle*i+45,360)-45-22.5)<=22.5\n",
    "                zndx = pndx & (dd_bins == dbin+1)\n",
    "                zz = np.zeros([npts,npts])\n",
    "                zz[zndx] = 1\n",
    "                yy[ndx,...,i,dbin] = zz\n",
    "    return yy\n",
    "    \n",
    "    \n",
    "\n",
    "def extract_patches(img,locs,psz):\n",
    "    zz = []\n",
    "    pad_arg = [(psz,psz),(psz,psz),(0,0)]\n",
    "    locs = np.round(locs).astype('int')\n",
    "    for ndx in range(img.shape[0]):\n",
    "        pimg = np.pad(img[ndx,...],pad_arg,'constant')\n",
    "        zz.append( pimg[(locs[ndx,1]+psz-psz/2):(locs[ndx,1]+psz+psz/2),\n",
    "                         (locs[ndx,0]+psz-psz/2):(locs[ndx,0]+psz+psz/2),:]);\n",
    "    return np.array(zz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_shape_prior(conf):\n",
    "#     global shape_prior\n",
    "    L = h5py.File(conf.labelfile,'r')\n",
    "    \n",
    "    if 'pts' in L:\n",
    "        pts = np.array(L['pts'])\n",
    "        v = conf.view\n",
    "    else:\n",
    "        pp = np.array(L['labeledpos'])\n",
    "        nmovie = pp.shape[1]\n",
    "        pts = np.zeros([0,conf.n_classes,2])\n",
    "        v = 0\n",
    "        for ndx in range(nmovie):\n",
    "            curpts = np.array(L[pp[0,ndx]])\n",
    "            frames = np.where(np.invert( np.any(np.isnan(curpts),axis=(1,2))))[0]\n",
    "            nptsPerView = np.array(L['cfg']['NumLabelPoints'])[0,0]\n",
    "            pts_st = int(conf.view*nptsPerView)\n",
    "            selpts = pts_st + conf.selpts\n",
    "            curlocs = curpts[:,:,selpts]\n",
    "            curlocs = curlocs[frames,:,:]\n",
    "            curlocs = curlocs.transpose([0,2,1])\n",
    "            pts = np.append(pts,curlocs[:,:,:],axis=0)\n",
    "    shape_prior = np.mean(shape_from_locs(pts)>0,axis=0)\n",
    "    return shape_prior\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreEval(sess,evalsaver,restore,conf,feed_dict):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.eval2outname)\n",
    "    latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                        latest_filename = conf.eval2ckptname)\n",
    "    sess.run(tf.global_variables_initializer(),feed_dict=feed_dict)\n",
    "    if not latest_ckpt or not restore:\n",
    "        evalstartat = 0\n",
    "        print(\"Not loading Eval variables. Initializing them\")\n",
    "    else:\n",
    "        evalsaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "        matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "        evalstartat = int(matchObj.group(1))+1\n",
    "        print(\"Loading eval variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "    return  evalstartat\n",
    "    \n",
    "\n",
    "def saveEval(sess,evalsaver,step,conf):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.eval2outname)\n",
    "    evalsaver.save(sess,outfilename,global_step=step,\n",
    "               latest_filename = conf.eval2ckptname)\n",
    "    print('Saved state to %s-%d' %(outfilename,step))\n",
    "\n",
    "def createEvalSaver(conf):\n",
    "    evalsaver = tf.train.Saver(var_list = PoseTools.getvars('eval'),\n",
    "                                    max_to_keep=conf.maxckpt)\n",
    "    return evalsaver\n",
    "\n",
    "def initializeRemainingVars(sess,feed_dict):\n",
    "    varlist = tf.global_variables()\n",
    "#     for var in varlist:\n",
    "#         try:\n",
    "#             sess.run(tf.assert_variables_initialized([var]))\n",
    "#         except tf.errors.FailedPreconditionError:\n",
    "#             sess.run(tf.variables_initializer([var]))\n",
    "#             print('Initializing variable:%s'%var.name)\n",
    "\n",
    "#     with tf.variable_scope('',reuse=True):\n",
    "#         varlist = sess.run( tf.report_uninitialized_variables( tf.global_variables( ) ) )\n",
    "#         sess.run( tf.initialize_variables( list( tf.get_variable(name) for name in  varlist) ) )\n",
    "\n",
    "def print_gradients(sess,feed_dict,loss):\n",
    "    vv = tf.global_variables()\n",
    "    aa = [v for v in vv if not re.search('Adam|batch_norm|beta|scale[1-2]|scale0_[1-9][0-9]*|fc_[1-9][0-9]*|L[6-7]_[1-9][0-9]*|biases',v.name)]\n",
    "\n",
    "    grads = sess.run(tf.gradients(loss,aa),feed_dict=feed_dict)\n",
    "\n",
    "    wts = sess.run(aa,feed_dict=feed_dict)\n",
    "\n",
    "    grads_std = [g.std() for g in grads]\n",
    "    wts_std = [w.std() for w in wts]\n",
    "\n",
    "    grads_by_wts = [s/w for s,w in zip(grads_std,wts_std)]\n",
    "\n",
    "\n",
    "\n",
    "    bb = [[r,n.name] for r,n in zip(grads_by_wts,aa)]\n",
    "    for b,k,g in zip(bb,grads_std,wts_std):\n",
    "        print b,k,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalNetInit(conf):\n",
    "    \n",
    "    ph = createPlaceHolders(conf)\n",
    "    feed_dict = createFeedDict(ph,conf)\n",
    "    init_shape_prior(conf)\n",
    "    with tf.variable_scope('eval'):\n",
    "        out,out_dict = net_multi_conv_shape(ph,conf)\n",
    "#change 3 22022017        \n",
    "#         out,out_dict = net_multi_conv(ph,conf)\n",
    "    trainType = 0\n",
    "    queue = openDBs(conf,trainType=trainType)\n",
    "    if trainType == 1:\n",
    "        print \"Training with all the data!\"\n",
    "        print \"Validation data is same as training data!!!! \"\n",
    "    return ph,feed_dict,out,queue,out_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalTrain(conf,restore=True):\n",
    "    ph,feed_dict,out,queue,_ = poseEvalNetInit(conf)\n",
    "    feed_dict[ph['phase_train']] = True\n",
    "    evalSaver = createEvalSaver(conf) \n",
    "    \n",
    "    shape_prior = init_shape_prior(conf)\n",
    "    prior_tensor = tf.constant(shape_prior)\n",
    "    y_re = tf.reshape(ph['y'],[conf.batch_size,8,conf.n_classes])\n",
    "    selpt1 = conf.eval2_selpt1\n",
    "    selpt2 = conf.eval2_selpt2\n",
    "    wt_den = shape_prior[selpt1,:,:,0].transpose([1,0])\n",
    "    wt = tf.reduce_max( y_re/(wt_den+0.1),axis=(1,2))\n",
    "    loss = tf.reduce_sum( tf.reduce_sum((out-ph['y'])**2,axis=1)*wt)\n",
    "#     loss = tf.nn.l2_loss(out-ph['y'])\n",
    "    correct_pred = tf.cast(tf.equal(out>0.5,ph['y']>0.5),tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_pred)\n",
    "    \n",
    "#     tf.summary.scalar('cross_entropy',loss)\n",
    "#     tf.summary.scalar('accuracy',accuracy)\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate= \\\n",
    "                      ph['learning_rate']).minimize(loss)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         train_writer = tf.summary.FileWriter(conf.cachedir + '/eval2_train_summary',sess.graph)\n",
    "#         test_writer = tf.summary.FileWriter(conf.cachedir + '/eval2_test_summary',sess.graph)\n",
    "        data,coord,threads = createCursors(sess,queue,conf)\n",
    "        updateFeedDict(conf,'train',distort=True,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "        evalstartat = restoreEval(sess,evalSaver,restore,conf,feed_dict)\n",
    "        initializeRemainingVars(sess,feed_dict)\n",
    "        for step in range(evalstartat,conf.eval2_training_iters+1):\n",
    "            excount = step*conf.batch_size\n",
    "            cur_lr = conf.eval2_learning_rate * conf.gamma**math.floor(excount/conf.step_size)\n",
    "            feed_dict[ph['learning_rate']] = cur_lr\n",
    "            feed_dict[ph['phase_train']] = True\n",
    "            updateFeedDict(conf,'train',distort=True,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "            train_summary,_ = sess.run([merged,opt], feed_dict=feed_dict)\n",
    "#             train_writer.add_summary(train_summary,step)\n",
    "\n",
    "            if step % conf.display_step == 0:\n",
    "                updateFeedDict(conf,'train',sess=sess,distort=True,data=data,feed_dict=feed_dict,ph=ph)\n",
    "                feed_dict[ph['phase_train']] = False\n",
    "                train_loss,train_acc = sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "                numrep = int(conf.num_test/conf.batch_size)+1\n",
    "                val_loss = 0.\n",
    "                val_acc = 0.\n",
    "#                 val_acc_wt = 0.\n",
    "                for rep in range(numrep):\n",
    "                    updateFeedDict(conf,'val',distort=False,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "                    vloss,vacc = sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "                    val_loss += vloss\n",
    "                    val_acc += vacc\n",
    "#                     val_acc_wt += vacc_wt\n",
    "                val_loss = val_loss/numrep\n",
    "                val_acc = val_acc/numrep\n",
    "#                 val_acc_wt /= numrep\n",
    "                test_summary,_ = sess.run([merged,loss],feed_dict=feed_dict)\n",
    "#                 test_writer.add_summary(test_summary,step)\n",
    "                print 'Val -- Acc:{:.4f} Loss:{:.4f} Train Acc:{:.4f} Loss:{:.4f} Iter:{}'.format(\n",
    "                    val_acc,val_loss,train_acc,train_loss,step)\n",
    "#                 print_gradients(sess,feed_dict,loss)\n",
    "            if step % conf.save_step == 0:\n",
    "                saveEval(sess,evalSaver,step,conf)\n",
    "        print(\"Optimization Done!\")\n",
    "        saveEval(sess,evalSaver,step,conf)\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genLabels(rlocs,locs,conf):\n",
    "    d2locs = np.sqrt(((rlocs-locs[...,np.newaxis])**2).sum(-2))\n",
    "    ll = np.arange(1,conf.n_classes+1)\n",
    "    labels = np.tile(ll[:,np.newaxis],[d2locs.shape[0],1,d2locs.shape[2]])\n",
    "    labels[d2locs>conf.poseEvalNegDist] = -1.\n",
    "    labels[d2locs<conf.poseEvalNegDist] = 1.\n",
    "    labels = np.concatenate([labels[:,np.newaxis],1-labels[:,np.newaxis]],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genRandomNegSamples(bout,l7out,locs,conf,nsamples=10):\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "    rlocs[:,:,0,:] = np.random.randint(sz[1],size=locs.shape[0:2]+(nsamples,))\n",
    "    rlocs[:,:,1,:] = np.random.randint(sz[0],size=locs.shape[0:2]+(nsamples,))\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianPosSamples(bout,l7out,locs,conf,nsamples=10,maxlen = 4):\n",
    "    scale = conf.rescale*conf.pool_scale\n",
    "    sigma = float(maxlen)*0.5*scale\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(15*nsamples,))*sigma)\n",
    "    # remove rlocs that are far away.\n",
    "    dlocs = np.all( np.sqrt( (rlocs**2).sum(2))< (maxlen*scale),1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:])[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianNegSamples(bout,locs,conf,nsamples=10,minlen = 8):\n",
    "    sigma = minlen\n",
    "#     sz = (np.array(bout.shape[1:3])-1)*scale\n",
    "    sz = np.array(bout.shape[1:3])-1\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(5*nsamples,))*sigma)\n",
    "    # remove rlocs that are small.\n",
    "    dlocs = np.sqrt( (rlocs**2).sum(2)).sum(1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:]> (minlen*conf.n_classes) )[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genMovedNegSamples(bout,locs,conf,nsamples=10,minlen=8):\n",
    "    # Add same x and y to locs\n",
    "    \n",
    "    minlen = float(minlen)/2\n",
    "    maxlen = 2*minlen\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "#     sz = (np.array(bout.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "    sz = np.array(bout.shape[1:3])-1\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        rx = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "        ry = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "\n",
    "        rlocs[curi,:,0,:] = locs[curi,:,0,np.newaxis] + rx\n",
    "        rlocs[curi,:,1,:] = locs[curi,:,1,np.newaxis] + ry\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genNMovedNegSamples(locs,conf,minlen=8):\n",
    "    # Move a random number of points.\n",
    "    minlen = float(minlen)\n",
    "    maxlen = 2*minlen\n",
    "    minlen = 0\n",
    "    \n",
    "    rlocs = copy.deepcopy(locs)\n",
    "    sz = conf.imsz\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        curN = np.random.randint(conf.n_classes)\n",
    "        for rand_point in np.random.choice(conf.n_classes,size=[curN,],replace=False):\n",
    "            rx = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "            ry = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "\n",
    "            rlocs[curi,rand_point,0] = locs[curi,rand_point,0] + rx\n",
    "            rlocs[curi,rand_point,1] = locs[curi,rand_point,1] + ry\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0] = xlocs\n",
    "    ylocs = rlocs[:,:,1]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genNegSamples(locs,conf,minlen=8):\n",
    "    return genNMovedNegSamples(locs,conf,minlen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
